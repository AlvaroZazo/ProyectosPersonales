{"cells":[{"cell_type":"markdown","metadata":{"id":"U9h-Nf5YU72B"},"source":["# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n","\n","* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n","\n","\n","* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (./data/corpus_mundo_today.csv).\n","\n","\n","* Este CSV esta formado por 3 campos que son:\n","    - Tema\n","    - Título\n","    - Texto\n","    \n","    \n","* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n","\n","\n","## 1.- Ejercicios de Nomalización:\n","\n","* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n","<span></span><br><br>\n","    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n","        * **input**: lista de documentos (lista de Strings).\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n","<span></span><br><br>\n","    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n","        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n","<span></span><br><br>\n","    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n","        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n","<span></span><br><br>\n","    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n","        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n"," <span></span><br><br>       \n","    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n","        * **input**: lista de documentos (lista de Strings).\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n","\n","\n","## 2.- Ejercicios de BoW:\n","\n","* Aprovechando la normalización realizada anteriormente se pide:\n","\n","    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n","        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n","        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n","        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n","<span></span><br><br>\n","    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n","<span></span><br><br>   \n","    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n","    \n","<hr>"]},{"cell_type":"markdown","metadata":{"id":"PAdC0k9cU72J"},"source":["## 1.- Ejercicios de Nomalización:\n","\n","* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qjipvE89U72K","executionInfo":{"status":"error","timestamp":1646925718382,"user_tz":-60,"elapsed":349,"user":{"displayName":"ALVARO ALVAREZ ZAZO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13862881707039858072"}},"outputId":"be8353a6-6997-40e4-8eea-d9f21ac40f31","colab":{"base_uri":"https://localhost:8080/","height":237}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3cb10a26ec95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocs_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/corpus_mundo_today.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfile_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'||'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/corpus_mundo_today.csv'"]}],"source":["docs_file = './data/corpus_mundo_today.csv'\n","docs_list = list()\n","file_txt = open(docs_file, encoding=\"utf8\").read()\n","for line in file_txt.split('\\n'):\n","    line = line.split('||')\n","    docs_list.append(line[1] + ' ' + line[2])\n","docs_list = docs_list[1:] # Elimino la cabecera del fichero"]},{"cell_type":"markdown","metadata":{"id":"BOgbtXsEU72M"},"source":["#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n","\n","* **input**: lista de documentos (lista de Strings).\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcPMSPiQU72N"},"outputs":[],"source":["def tokenization(docs_list):\n","    # TODO\n","    return docs_list"]},{"cell_type":"markdown","metadata":{"id":"dER4e5kTU72O"},"source":["#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n","\n","* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ALrCk5DU72P"},"outputs":[],"source":["def remove_words(docs):\n","    # TODO\n","    return docs"]},{"cell_type":"markdown","metadata":{"id":"woj_T0tdU72P"},"source":["#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n","\n","* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnUoy6HlU72Q"},"outputs":[],"source":["def lematization(docs):\n","    # TODO\n","    return docs"]},{"cell_type":"markdown","metadata":{"id":"8-68ZM-lU72Y"},"source":["#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n","\n","* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9DfV8Q0U72Z"},"outputs":[],"source":["def filter_words(docs):\n","    # TODO\n","    return docs"]},{"cell_type":"markdown","metadata":{"id":"2xBrFjGPU72Z"},"source":["#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n","\n","* **input**: lista de documentos (lista de Strings).\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_Dym7ACU72a"},"outputs":[],"source":["def normalization(docs_list):\n","    corpus = tokenization(docs_list)\n","    corpus = remove_words(corpus)\n","    corpus = lematization(corpus)\n","    corpus = filter_words(corpus)\n","    return corpus\n","\n","corpus = normalization(docs_list)\n","print(corpus[0])"]},{"cell_type":"markdown","metadata":{"id":"YvTeQCohU72b"},"source":["<hr>\n","\n","\n","## 2.- Ejercicios de BoW:\n","\n","#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n","\n","* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n","* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n","* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2m3jCbnU72b"},"outputs":[],"source":["def drop_less_frecuency_words(corpus, n):\n","    # TODO\n","    return corpus\n","\n","corpus = drop_less_frecuency_words(corpus, 10)\n","print(corpus[0])"]},{"cell_type":"markdown","metadata":{"id":"bMGziewKU72c"},"source":["#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw-4DD-NU72c"},"outputs":[],"source":["import nltk\n","import gensim\n","\n","# TODO"]},{"cell_type":"markdown","metadata":{"id":"IpVbkSVAU72d"},"source":["#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hyZNq0DU72d"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# TODO"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"06_Ejercicio1_Normalizacion_BoW.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}